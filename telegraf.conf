# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"


# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "30s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "30s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
   debug = false
  ## Log only error level messages.
   quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
   logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
   logfile = "/var/log/telegraf/telegraf.log"

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
   logfile_rotation_max_size = "10MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
   logfile_rotation_max_archives = 5

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# Configuration for sending metrics to InfluxDB
[[outputs.influxdb]]
  ## The full HTTP or UDP URL for your InfluxDB instance.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  # urls = ["unix:///var/run/influxdb.sock"]
  # urls = ["udp://127.0.0.1:8089"]
  urls = ["http://<your-influxdb-ip>:8086"]

  ## The target database for metrics; will be created as needed.
  ## For UDP url endpoint database needs to be configured on server side.
  database = "<your-db-name>"

  ## The value of this tag will be used to determine the database.  If this
  ## tag is not set the 'database' option is used as the default.
  # database_tag = ""

  ## If true, the 'database_tag' will not be included in the written metric.
  # exclude_database_tag = false

  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using
  ## Telegraf with a user without permissions to create databases or when the
  ## database already exists.
  # skip_database_creation = false

  ## Name of existing retention policy to write to.  Empty string writes to
  ## the default retention policy.  Only takes effect when using HTTP.
  # retention_policy = ""

  ## The value of this tag will be used to determine the retention policy.  If this
  ## tag is not set the 'retention_policy' option is used as the default.
  # retention_policy_tag = ""

  ## If true, the 'retention_policy_tag' will not be included in the written metric.
  # exclude_retention_policy_tag = false

  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all".
  ## Only takes effect when using HTTP.
  # write_consistency = "any"

  ## Timeout for HTTP messages.
  # timeout = "5s"

  ## HTTP Basic Auth
  username = "<db-user>"
  password = "<db-password>"

  ## HTTP User-Agent
  # user_agent = "telegraf"

  ## UDP payload size is the maximum packet size to send.
  # udp_payload = "512B"

  ## Optional TLS Config for use on HTTP connections.
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  ## Use TLS but skip chain & host verification
  # insecure_skip_verify = false

  ## HTTP Proxy override, if unset values the standard proxy environment
  ## variables are consulted to determine which proxy, if any, should be used.
  # http_proxy = "http://corporate.proxy:3128"

  ## Additional HTTP headers
  # http_headers = {"X-Special-Header" = "Special-Value"}

  ## HTTP Content-Encoding for write request body, can be set to "gzip" to
  ## compress body or "identity" to apply no encoding.
  # content_encoding = "gzip"

  ## When true, Telegraf will output unsigned integers as unsigned values,
  ## i.e.: "42u".  You will need a version of InfluxDB supporting unsigned
  ## integer values.  Enabling this option will result in field type errors if
  ## existing data has been written.
  # influx_uint_support = false


###############################################################################
#                               INPUT  PLUGINS                                #
###############################################################################

# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = false
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  report_active = true

# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
  devices = ["sda2", "md0", "nvme0n1", "nvme1n1", "sdb1", "sdc1", "sdd1", "sde1", "sdf1", "sdg1", "sdh1", "sdi1", "md1", "sdj1", "sdk1", "sdl1", "sdm1", "md2", "sdn1", "md3", "sdo1", "md1", "md4", "sdp1", "sdq1"]
  ## On systems which support it, device metadata can be added in the form of tags.
  ## 'udevadm info -q property -n /dev/sda'
  ## Note: Most, but not all, udev properties can be accessed this way. Properties
  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  device_tags = ["ID_MODEL", "ID_REVISION", "ID_FS_LABEL"]
  ## Using the same metadata source as device_tags, you can also customize the
  ## name of the device via templates.
  ## The 'name_templates' parameter is a list of templates to try and apply to
  ## the device. The template may contain variables in the form of '$PROPERTY' or
  ## '${PROPERTY}'. The first template which does not contain any variables not
  ## present for the device is used as the device name tag.
  ## The typical use case is for LVM volumes, to get the VG/LV name instead of
  ## the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]

[[inputs.disk]]
  interval = "10m"
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]
  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

[[inputs.mem]]

[[inputs.temp]]

[[inputs.hddtemp]]

# Monitor process cpu and memory usage
[[inputs.procstat]]
  ## PID file to monitor process
  # pid_file = "/var/run/nginx.pid"
  ## executable name (ie, pgrep <exe>)
  # exe = "nginx"
  ## pattern as argument for pgrep (ie, pgrep -f <pattern>)
#  pattern = "plots create"
  pattern = "chia_plot"
  ## user as argument for pgrep (ie, pgrep -u <user>)
  # user = "nginx"
  ## Systemd unit name
  # systemd_unit = "nginx.service"
  ## CGroup name or path
  # cgroup = "systemd/system.slice/nginx.service"
  ## override for process_name
  ## This is optional; default is sourced from /proc/<pid>/status
  process_name = "chia-plot"
  ## Field name prefix
  # prefix = ""
  ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.
  # mode = "irix"
  ## Method to use when finding process IDs.  Can be one of 'pgrep', or
  ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while
  ## the native finder performs the search directly in a manor dependent on the
  ## platform.  Default is 'pgrep'
  # pid_finder = "pgrep"

#nvme drives temperture
[[inputs.exec]]
  commands = [
    "sh /home/nvme0_temp.sh",
    "sh /home/nvme1_temp.sh",
  ]
  timeout = "5s"
  data_format = "influx"

[[inputs.exec]]
  interval = "30m"
  commands = [
    "bash /home/tgf_chia.sh",
  ]
  timeout = "5s"
  data_format = "influx"

#[[inputs.tail]]
#  files = ["../Swar-Chia-Plot-Manager/logs/*.log"]
#  from_beginning = false
#   watch_method = "inotify"
#   max_undelivered_lines = 1000
#   name_override = "plot_logs"
#   data_format = "grok"
#   grok_timezone = "Local"
#
#  grok_patterns = ["Total time = %{NUMBER:plot_total_time:float} seconds. CPU \\((\\d+\\.\\d+)%\\) %{TS_UNIX:timestamp:ts-ansic}",
#    "Time for phase %{NUMBER:plot_phase:tag} = %{NUMBER:plot_phase_time:float} seconds. CPU \\((\\d+\\.\\d+)%\\) %{TS_UNIX:timestamp:ts-ansic}",
#    "Copy time = %{NUMBER:plot_copy_time:float} seconds. CPU \\((\\d+\\.\\d+)%\\) %{TS_UNIX:timestamp:ts-ansic}"]
#
#  grok_custom_patterns = '''
#    TS_UNIX %{DAY} %{MONTH}( |  )%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{YEAR}
#  '''
#  [inputs.tail.tags]
#    job = "plot-logs"

[[inputs.tail]]
  files = ["../plot-logs/*.log"]
  from_beginning = false
   watch_method = "inotify"
   max_undelivered_lines = 1000
   name_override = "plot_logs"
   data_format = "grok"
   grok_timezone = "Local"
 # 2021-06-11T15:10:58.561362Z Phase 2 took 936.626 sec
 # Total plot creation time was 14978.6 sec
 # Copy to /mnt/hdd8/plot-k32-2021-06-11-10-47-0ba397db27bc2c93518452bf7fe79b3a1f05cc14254bd1c4cc83e7e9e8c1877e.plot finished, took 414.121 sec, 250.636 MB/s avg.
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02T15:04:05.999Z"} Total plot creation time was %{NUMBER:plot_total_time:float} sec',
    '%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02T15:04:05.999Z"} Phase %{NUMBER:plot_phase:tag} took %{NUMBER:plot_phase_time:float} sec',
    'finished, took %{NUMBER:plot_copy_time:float} sec']
  [inputs.tail.tags]
    job = "plot-logs"

[[inputs.tail]]
  files = ["../.chia/mainnet/log/debug.log"]
   from_beginning = false
   watch_method = "inotify"
   max_undelivered_lines = 1000
   name_override = "farm_logs"
   data_format = "grok"
   grok_timezone = "Local"

## 2021-05-28T09:17:50.145 harvester chia.harvester.harvester: INFO     1 plots were eligible for farming 7d3ea83990... Found 0 proofs. Time: 0.14002 s. Total 257 plots"
  grok_patterns = ['''%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02T15:04:05.999"} harvester chia.harvester.harvester: INFO     %{NUMBER:eligible_plots:int} plots were eligible for farming ([0-9a-f]{10})+\.{3} Found %{NUMBER:proofs:int} proofs. Time: %{NUMBER:proof_time:float} s. Total (\d+) plots''']

#space pool api
[[inputs.http]]
  interval = "15m"
  urls = [ "https://mainnet.pool.space/api/farms/<your-launcher-id>" ]
  data_format = "json"

#count pool plots
[[inputs.filecount]]
  interval = "15m"
  directories = ["/mnt/hdd*/pool"]
  name = "*"
  ## Count files in subdirectories. Defaults to true.
  recursive = false
  ## Only count regular files. Defaults to true.
  regular_only = true
  size = "101GiB"
